# Supply Chain Vulunerability

- [I Know What You Asked: Prompt Leakage via KV-Cache Sharing in Multi-Tenant LLM Serving](https://arxiv.org/abs/2409.20002v2), explores how shared KV-cache mechanisms in multi-tenant LLM deployments can lead to prompt leakage
- [Prompt-to-SQL Injections in LLM-Integrated Web Applications: Risks and Defenses](https://syssec.dpss.inesc-id.pt/papers/pedro_icse25.pdf), investigates the security vulnerabilities introduced by LLM-generated SQL queries and proposes mitigation techniques
- [My Model is Malware to You: Transforming AI Models into Malware by Abusing TensorFlow APIs](https://github.com/ZJU-SEC/TensorAbuse), demonstrates how AI models can be repurposed as malware through abuse of TensorFlow APIs
- [Models Are Codes: Towards Measuring Malicious Code Poisoning Attacks on Pre-trained Model Hubs](https://arxiv.org/pdf/2409.09368), studies how malicious code injections in pre-trained model repositories can compromise AI model security
- [Demystifying RCE Vulnerabilities in LLM-Integrated Apps](https://arxiv.org/pdf/2309.02926), examines remote code execution (RCE) risks in applications integrated with LLMs 
- [Cracks in The Stack: Hidden Vulnerabilities and Licensing Risks in LLM Pre-Training Datasets](https://arxiv.org/pdf/2501.02628), identifies security risks and legal concerns in datasets used for training large language models
- [We Have a Package for You! A Comprehensive Analysis of Package Hallucinations by Code Generating LLMs](https://arxiv.org/pdf/2406.10279), analyzes how LLMs hallucinate non-existent software packages and the implications for software supply chain security
- [An Empirical Study of Artifacts and Security Practices in the Pre-trained Model Supply Chain](https://wenxin-jiang.github.io/files/publications/JiangSynovicSethiIndarapuHyattSchorlemmerThiruvathukalDavis-PTMSupplyChain-SCORED22.pdf), investigates security best practices and vulnerabilities in the pre-trained model supply chain
- [Naming Practices of Pre-Trained Models in Hugging Face](https://arxiv.org/pdf/2310.01642), explores the naming conventions of pre-trained models and their impact on discoverability and security
- [Towards Semantic Versioning of Open Pre-trained Language Model Releases on Hugging Face](https://arxiv.org/pdf/2409.10472), proposes a versioning framework for tracking updates and changes in open LLM releases
- [A New Era in LLM Security: Exploring Security Concerns in Real-World LLM-based Systems](https://arxiv.org/pdf/2402.18649), examines real-world security threats faced by LLM-integrated systems and their countermeasures
- [Large Language Model Supply Chain: A Research Agenda](https://arxiv.org/pdf/2404.12736), presents a roadmap for addressing security and integrity risks in the LLM supply chain
- [Lifting the Veil on the Large Language Model Supply Chain: Composition, Risks, and Mitigations](https://arxiv.org/pdf/2410.21218), analyzes the structure and security challenges of the LLM supply chain
- [Large Language Model Supply Chain: Open Problems From the Security Perspective](https://arxiv.org/pdf/2411.01604), outlines key security risks and unresolved challenges in securing LLM supply chains
- [Auditing Prompt Caching in Language Model APIs](https://arxiv.org/abs/2502.07776), examines security and privacy risks associated with prompt caching in LLM APIs
- [An Engorgio Prompt Makes Large Language Model Babble on](https://arxiv.org/abs/2412.19394), explores how certain prompts can cause LLMs to generate excessive and uncontrollable outputs
- [Crabs: Consuming Resource via Auto-generation for LLM-DoS Attack under Black-box Settings](https://arxiv.org/abs/2412.13879), introduces a denial-of-service attack against LLMs that exploits auto-generation mechanisms
- [AttentionBreaker: Adaptive Evolutionary Optimization for Unmasking Vulnerabilities in LLMs through Bit-Flip Attacks](https://arxiv.org/abs/2411.13757), presents a novel attack method using bit-flipping to identify vulnerabilities in LLMs
- [Towards Evaluations-based Safety Cases for AI Scheming](https://arxiv.org/abs/2411.03336), discusses a framework for evaluating AI safety cases with a focus on strategic AI behaviors
- [Safeguard is a Double-edged Sword: Denial-of-service Attack on Large Language Models](https://arxiv.org/abs/2410.02916), investigates how security mechanisms can be exploited to perform denial-of-service attacks on LLMs
- [Exploiting LLM Quantization](https://arxiv.org/abs/2405.18137), examines security implications of quantization techniques used in large language models
- [Attacks on Third-Party APIs of Large Language Models](https://arxiv.org/abs/2404.16891), analyzes security threats targeting third-party API integrations with LLMs
- [Towards AI Safety: A Taxonomy for AI System Evaluation](https://arxiv.org/html/2404.05388v1), proposes a taxonomy to systematically evaluate AI system safety
- [SecGPT: An Execution Isolation Architecture for LLM-Based Systems](https://arxiv.org/abs/2403.04960), introduces a security architecture to isolate LLM execution from potential threats
- [Here Comes The AI Worm: Unleashing Zero-click Worms that Target GenAI-Powered Applications](https://arxiv.org/abs/2403.02817), describes a new type of AI worm capable of targeting generative AI applications
- [A New Era in LLM Security: Exploring Security Concerns in Real-World LLM-based Systems](https://arxiv.org/abs/2402.18649), surveys security vulnerabilities and risks in LLM-powered real-world applications
- [A First Look at GPT Apps: Landscape and Vulnerability](https://arxiv.org/abs/2402.15105), provides an overview of security challenges in GPT-based applications
