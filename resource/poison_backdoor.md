# Poison & Backdoor

- [Gungnir: Exploiting Stylistic Features in Images for Backdoor Attacks on Diffusion Models](https://arxiv.org/abs/2502.20650), investigating the use of stylistic manipulation in images for conducting backdoor attacks on diffusion models. 
- [The RAG Paradox: A Black-Box Attack Exploiting Unintentional Vulnerabilities in Retrieval-Augmented Generation Systems](https://arxiv.org/abs/2502.20995), uncovering adversarial techniques that target unintended vulnerabilities in retrieval-augmented generation (RAG) systems.  
- [ELBA-Bench: An Efficient Learning Backdoor Attacks Benchmark for Large Language Models](https://arxiv.org/abs/2502.18511), introducing a benchmarking framework designed to evaluate backdoor attack strategies in large language models (LLMs).  
- [DemonAgent: Dynamically Encrypted Multi-Backdoor Implantation Attack on LLM-based Agent](https://arxiv.org/abs/2502.12575), presenting a multi-backdoor attack that leverages dynamic encryption to implant stealthy backdoors into LLM-based agents.  
- [BackdoorDM: A Comprehensive Benchmark for Backdoor Learning in Diffusion Model](https://arxiv.org/abs/2502.11798), providing an extensive benchmark to study backdoor vulnerabilities in diffusion models.  
- [THEMIS: Regulating Textual Inversion for Personalized Concept Censorship](https://www.ndss-symposium.org/ndss-paper/themis-regulating-textual-inversion-for-personalized-concept-censorship/), investigating regulatory techniques for textual inversion to enable personalized concept censorship in diffusion models.  
- [A Survey on Backdoor Threats in Large Language Models (LLMs): Attacks, Defenses, and Evaluations](https://arxiv.org/abs/2502.05224), offering a comprehensive survey on backdoor threats, attack methods, defense mechanisms, and evaluation techniques for LLMs.  
- [Exploring the Security Threats of Knowledge Base Poisoning in Retrieval-Augmented Code Generation](https://arxiv.org/abs/2502.03233), analyzing the security risks associated with poisoning knowledge bases in retrieval-augmented code generation models.  
- [DarkMind: Latent Chain-of-Thought Backdoor in Customized LLMs](https://arxiv.org/abs/2501.18617), demonstrating how chain-of-thought reasoning can be exploited to embed backdoors in customized LLMs.  
- [CL-attack: Textual Backdoor Attacks via Cross-Lingual Triggers](https://arxiv.org/abs/2412.19037), exploring cross-lingual trigger-based textual backdoor attacks on LLMs.
- [Towards Faithful XAI Evaluation via Generalization-Limited Backdoor Watermark](https://openreview.net/forum?id=cObFETcoeW), introducing a watermarking approach to evaluate explainable AI under limited generalization conditions.  
- [Towards Reliable and Efficient Backdoor Trigger Inversion via Decoupling Benign Features](https://openreview.net/forum?id=Tw9wemV6cb), proposing a method to improve backdoor trigger inversion by separating benign and malicious features.  
- [BaDExpert: Extracting Backdoor Functionality for Accurate Backdoor Input Detection](https://openreview.net/forum?id=s56xikpD92), exploring how to extract and leverage backdoor functionality for more precise backdoor input detection.  
- [Backdoor Secrets Unveiled: Identifying Backdoor Data with Optimized Scaled Prediction Consistency](https://openreview.net/forum?id=1OfAO2mes1), developing a method to identify backdoor data using an optimized scaled prediction consistency framework.  
- [Adversarial Feature Map Pruning for Backdoor](https://openreview.net/forum?id=IOEEDkla96), investigating the effectiveness of adversarial feature map pruning in removing backdoors from neural networks.  
- [Safe and Robust Watermark Injection with a Single OoD Image](https://openreview.net/forum?id=PCm1oT8pZI), presenting a novel watermarking approach for model security using a single out-of-distribution image.  
- [Efficient Backdoor Attacks for Deep Neural Networks in Real-world Scenarios](https://openreview.net/forum?id=vRyp2dhEQp), exploring efficient backdoor attack techniques tailored for real-world deployment scenarios.  
- [Backdoor Contrastive Learning via Bi-level Trigger Optimization](https://openreview.net/forum?id=oxjeePpgSP), introducing a bi-level optimization method for embedding backdoors in contrastive learning models.  
- [BadEdit: Backdooring Large Language Models by Model Editing](https://openreview.net/forum?id=duZANm2ABX), proposing a method to implant backdoors in large language models through targeted model editing.  
- [Backdoor Federated Learning by Poisoning Backdoor-Critical Layers](https://openreview.net/forum?id=AJBGSVSTT2), demonstrating how backdoor-critical layers in federated learning can be compromised via poisoning.  
- [Poisoned Forgery Face: Towards Backdoor Attacks on Face Forgery Detection](https://openreview.net/forum?id=8iTpB4RNvP), analyzing how backdoor attacks can compromise deepfake detection systems.  
- [Influencer Backdoor Attack on Semantic Segmentation](https://openreview.net/forum?id=VmGRoNDQgJ), examining how backdoor attacks can manipulate influencer-based semantic segmentation models.  
- [Rethinking Backdoor Attacks on Dataset Distillation: A Kernel Method Perspective](https://openreview.net/forum?id=iCNOK45Csv), providing a kernel-based analysis of backdoor attacks in dataset distillation.  
- [Universal Backdoor Attacks](https://openreview.net/forum?id=3QkzYBSWqL), proposing a universal framework for deploying backdoor attacks across various deep learning models.  
- [Demystifying Poisoning Backdoor Attacks from a Statistical Perspective](https://openreview.net/forum?id=BPHcEpGvF8), offering a statistical analysis of poisoning-based backdoor attacks and their mitigation.  
- [BadChain: Backdoor Chain-of-Thought Prompting for Large Language Models](https://openreview.net/forum?id=c93SBwz1Ma), demonstrating how chain-of-thought reasoning can be leveraged for backdoor attacks on LLMs.  
- [Rethinking CNNâ€™s Generalization to Backdoor Attack from Frequency Domain](https://openreview.net/forum?id=mYhH0CDFFa), investigating CNN vulnerabilities to backdoors from a frequency domain perspective.  
- [Like Oil and Water: Group Robustness Methods and Poisoning Defenses Don't Mix](https://openreview.net/forum?id=rM9VJPB20F), analyzing the incompatibility between robustness techniques and poisoning defenses.  
- [VDC: Versatile Data Cleanser for Detecting Dirty Samples via Visual-Linguistic Inconsistency](https://openreview.net/forum?id=ygxTuVz9eU), proposing a data cleansing approach using visual-linguistic inconsistency to detect backdoor samples.  
- [Chameleon: Increasing Label-Only Membership Leakage with Adaptive Poisoning](https://openreview.net/forum?id=4DoSULcfG6), examining how adaptive poisoning can enhance label-only membership inference attacks.  
- [Universal Jailbreak Backdoors from Poisoned Human Feedback](https://openreview.net/forum?id=GxCGsxiAaK), showcasing how poisoned human feedback can introduce universal jailbreak backdoors in LLMs.  
- [Teach LLMs to Phish: Stealing Private Information from Language Models](https://openreview.net/forum?id=qo21ZlfNu6), demonstrating methods for extracting private information from LLMs using adversarial techniques.  
