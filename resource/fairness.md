# Fairness

- [ModScan: Measuring Stereotypical Bias in Large Vision-Language Models from Vision and Language Modalities](https://arxiv.org/abs/2410.06967), a study that evaluates stereotypical biases in large vision-language models by analyzing both their visual and textual outputs
- [Pro-Woman, Anti-Man? Identifying Gender Bias in Stance Detection](https://aclanthology.org/2024.findings-acl.192/), research exploring gender bias in stance detection models, revealing tendencies to favor women over men in certain contexts [<img src="https://github.com/FortAwesome/Font-Awesome/blob/6.x/svgs/brands/github.svg" alt="Code" width="15" height="15">](https://github.com/chuchun8/GenderStance)
- [Social Debiasing for Fair Multi-modal LLMs](https://arxiv.org/abs/2408.06569), a novel approach to mitigating social biases in multi-modal LLMs by leveraging debiasing techniques across different modalities
- [GenderAlign: An Alignment Dataset for Mitigating Gender Bias in Large Language Models](https://arxiv.org/abs/2406.13925), introduces a dataset aimed at improving fairness in LLMs by aligning them with unbiased gender representations
- [FairMonitor: A Dual-framework for Detecting Stereotypes and Biases in Large Language Models](https://arxiv.org/abs/2405.03098), proposes a two-pronged framework for identifying and mitigating biases in LLMs through systematic evaluation and intervention
- [Born With a Silver Spoon? Investigating Socioeconomic Bias in Large Language Models](https://arxiv.org/abs/2403.14633), an analysis of how LLMs reflect and propagate socioeconomic biases, potentially reinforcing class disparities
- [Protected group bias and stereotypes in Large Language Models](https://arxiv.org/abs/2403.14727), explores how LLMs encode biases against protected demographic groups, influencing their outputs
- [Locating and Mitigating Gender Bias in Large Language Models](https://arxiv.org/abs/2403.14409), a study focused on identifying and reducing gender biases in LLMs through targeted interventions
- [Steering LLMs Towards Unbiased Responses: A Causality-Guided Debiasing Framework](https://arxiv.org/abs/2403.08743), presents a causality-driven approach to training LLMs for more neutral and fair responses
- [Bias-Augmented Consistency Training Reduces Biased Reasoning in Chain-of-Thought](https://arxiv.org/abs/2403.05518), investigates how incorporating bias-aware training can enhance LLMs' reasoning fairness
- [Exploiting Cultural Biases via Homoglyphs in Text-to-Image Synthesis](https://arxiv.org/abs/2209.08891), explores how homoglyphs can be used to reveal cultural biases in text-to-image synthesis models
- [Moral Mimicry: Large Language Models Produce Moral Rationalizations Tailored to Political Identity](https://arxiv.org/abs/2209.12106), investigates how LLMs generate moral justifications aligned with political ideologies
- [Auto-Debias: Debiasing Masked Language Models with Automated Biased Prompts](https://aclanthology.org/2022.acl-long.72/), introduces a method to automatically generate biased prompts for debiasing masked language models [<img src="https://github.com/FortAwesome/Font-Awesome/blob/6.x/svgs/brands/github.svg" alt="Code" width="15" height="15">](https://github.com/Irenehere/Auto-Debias)
- [Mitigating Gender Bias in Distilled Language Models via Counterfactual Role Reversal](https://arxiv.org/abs/2203.12574), proposes a technique to counteract gender biases in distilled LLMs using counterfactual role reversal
- [Mitigating Political Bias in Language Models Through Reinforced Calibration](https://arxiv.org/abs/2104.14795), introduces a reinforcement learning approach to reduce political bias in LLMs
- [Bias Out-of-the-Box: An Empirical Analysis of Intersectional Occupational Biases in Popular Generative Language Models](https://arxiv.org/abs/2102.04130), examines how generative LLMs exhibit occupational biases across intersectional identities
- [Persistent Anti-Muslim Bias in Large Language Models](https://arxiv.org/abs/2101.05783), highlights the ongoing issue of anti-Muslim bias in LLM outputs and potential mitigation strategies
- [Man is to Computer Programmer as Woman is to Homemaker? Debiasing Word Embeddings](https://arxiv.org/abs/1607.06520), introduces a method to identify and remove gender bias from word embeddings using a projection-based approach [![GitHub](https://github.com/tolga-b/debiaswe)]
- [Semantics derived automatically from language corpora contain human-like biases](https://science.sciencemag.org/content/356/6334/183), demonstrates that word embeddings trained on large corpora capture human-like biases, reflecting real-world stereotypes
- [Attenuating Biases in Word Vectors](http://proceedings.mlr.press/v89/dev19a.html), proposes a method to reduce bias in word embeddings while preserving useful semantic information
- [Gender Bias in Contextualized Word Embeddings](https://www.aclweb.org/anthology/N19-1064), investigates gender bias in contextualized word embeddings (BERT, ELMo) and its impact on downstream NLP tasks
- [Black is to Criminal as Caucasian is to Police: Detecting and Removing Multiclass Bias in Word Embeddings](https://www.aclweb.org/anthology/N19-1062), identifies racial and gender biases in word embeddings and proposes a debiasing method
- [Towards Understanding Linear Word Analogies](https://www.aclweb.org/anthology/P19-1315), analyzes how linear word analogies in embeddings relate to real-world biases
- [Understanding Undesirable Word Embedding Associations](https://www.aclweb.org/anthology/P19-1166), explores the biases hidden in word embeddings and methods to mitigate them
- [Gender Bias in Multilingual Embeddings and Cross-Lingual Transfer](https://www.aclweb.org/anthology/2020.acl-main.260), evaluates gender bias in multilingual word embeddings and its impact on cross-lingual NLP tasks
- [Nurse is Closer to Woman than Surgeon? Mitigating Gender-Biased Proximities in Word Embeddings](https://www.aclweb.org/anthology/2020.tacl-1.32), proposes a method to correct biased proximity relationships in word embeddings
- [Measuring Bias in Contextualized Word Representations](https://www.aclweb.org/anthology/W19-3823), introduces a framework to quantify bias in contextualized word embeddings like BERT
- [Unmasking Contextual Stereotypes: Measuring and Mitigating BERT's Gender Bias](https://www.aclweb.org/anthology/2020.gebnlp-1.1), assesses BERT’s gender bias and suggests techniques to reduce stereotypical associations
- [Evaluating the Underlying Gender Bias in Contextualized Word Embeddings](https://www.aclweb.org/anthology/W19-3805), investigates how contextualized embeddings reflect and amplify gender biases
- [Evaluating Bias In Dutch Word Embeddings](https://www.aclweb.org/anthology/2020.gebnlp-1.6), examines bias in Dutch word embeddings and its effects on NLP applications
- [Learning Gender-Neutral Word Embeddings](https://arxiv.org/abs/1809.01496), introduces an approach to generate gender-neutral word embeddings while preserving linguistic expressiveness
- [Lipstick on a Pig: Debiasing Methods Cover up Systematic Gender Biases in Word Embeddings But do not Remove Them](https://www.aclweb.org/anthology/N19-1061), critiques existing debiasing methods and shows that bias remains even after debiasing
- [It’s All in the Name: Mitigating Gender Bias with Name-Based Counterfactual Data Substitution](https://www.aclweb.org/anthology/D19-1530), proposes using counterfactual name substitution to reduce gender bias in NLP models
- [Gender-preserving Debiasing for Pre-trained Word Embeddings](https://www.aclweb.org/anthology/P19-1160), develops a method to remove gender bias while maintaining gender-relevant word relationships
- [Debiasing Pre-trained Contextualised Embeddings](https://www.aclweb.org/anthology/2021.eacl-main.107), investigates methods to mitigate bias in pre-trained language models
- [Dictionary-based Debiasing of Pre-trained Word Embeddings](https://www.aclweb.org/anthology/2021.eacl-main.16), introduces a dictionary-based approach for bias removal in word embeddings
- [Conceptor Debiasing of Word Representations Evaluated on WEAT](https://www.aclweb.org/anthology/W19-3806), applies conceptor learning to debias word embeddings and evaluates it using the Word Embedding Association Test (WEAT)
- [Are We Consistently Biased? Multidimensional Analysis of Biases in Distributional Word Vectors](https://www.aclweb.org/anthology/S19-1010), analyzes multiple dimensions of bias in word embeddings
- [Gender Bias in Pretrained Swedish Embeddings](https://www.aclweb.org/anthology/W19-6104), evaluates and mitigates gender bias in Swedish NLP models
- [On Measuring Social Biases in Sentence Encoders](https://www.aclweb.org/anthology/N19-1063), develops techniques to measure social biases in sentence embedding models
- [Fair Is Better than Sensational: Man Is to Doctor as Woman Is to Doctor](https://www.aclweb.org/anthology/2020.cl-2.7), critiques analogy-based bias evaluation and proposes a fairness-based approach
- [Neutralizing Gender Bias in Word Embeddings with Latent Disentanglement and Counterfactual Generation](https://www.aclweb.org/anthology/2020.findings-emnlp.280), introduces a new framework for removing gender bias through counterfactual data generation
- [Debiasing knowledge graph embeddings](https://www.aclweb.org/anthology/2020.emnlp-main.595), examines biases in knowledge graph embeddings and methods for mitigating them
- [Assessing the Reliability of Word Embedding Gender Bias Measures](https://arxiv.org/abs/2109.04732), evaluates the robustness of existing gender bias measurement techniques
- [Harms of Gender Exclusivity and Challenges in Non-Binary Representation in Language Technologies](https://arxiv.org/abs/2108.12084), explores the exclusion of non-binary identities in NLP models and suggests strategies for inclusivity
