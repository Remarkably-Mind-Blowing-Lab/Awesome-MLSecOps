# Toolkit

- [NB Defense](https://nbdefense.ai), Secure Jupyter Notebooks  
- [Garak](https://github.com/leondz/garak), LLM vulnerability scanner  
- [Adversarial Robustness Toolbox](https://github.com/IBM/adversarial-robustness-toolbox), Library of defense methods for ML models against adversarial attacks  
- [MLSploit](https://github.com/mlsploit/), Cloud framework for interactive experimentation with adversarial machine learning research  
- [TensorFlow Privacy](https://github.com/tensorflow/privacy), Library of privacy-preserving machine learning algorithms and tools  
- [Foolbox](https://github.com/bethgelab/foolbox), Python toolbox for creating and evaluating adversarial attacks and defenses  
- [Advertorch](https://github.com/BorealisAI/advertorch), Python toolbox for adversarial robustness research  
- [Artificial Intelligence Threat Matrix](https://collaborativeaicontrols.github.io/ATM/), Framework for identifying and mitigating threats to machine learning systems  
- [Adversarial ML Threat Matrix](https://github.com/mitre/advmlthreatmatrix), Adversarial Threat Landscape for AI Systems  
- [CleverHans](https://github.com/cleverhans-lab/cleverhans), A library of adversarial examples and defenses for machine learning models  
- [AdvBox](https://github.com/advboxes/AdvBox), Toolbox to generate adversarial examples for multiple ML frameworks  
- [Audit AI](https://github.com/pymetrics/audit-ai), Bias Testing for Generalized Machine Learning Applications  
- [Deep Pwning](https://github.com/cchio/deep-pwning), Framework for evaluating ML model robustness against adversarial attacks  
- [Privacy Meter](https://github.com/privacytrustlab/ml_privacy_meter), Library to audit data privacy in ML algorithms  
- [TensorFlow Model Analysis](https://github.com/tensorflow/model-analysis), Library for analyzing, validating, and monitoring ML models in production  
- [PromptInject](https://github.com/agencyenterprise/PromptInject), Framework for assembling adversarial prompts  
- [TextAttack](https://github.com/QData/TextAttack), Python framework for adversarial attacks, data augmentation, and model training in NLP  
- [OpenAttack](https://github.com/thunlp/OpenAttack), Open-Source Package for Textual Adversarial Attack  
- [TextFooler](https://github.com/jind11/TextFooler), Model for Natural Language Attack on Text Classification and Inference  
- [Flawed Machine Learning Security](https://github.com/EthicalML/fml-security), ML security best practices and examples of vulnerabilities  
- [Adversarial Machine Learning CTF](https://github.com/arturmiller/adversarial_ml_ctf), CTF challenge showcasing security flaws in neural networks  
- [Damn Vulnerable LLM Project](https://github.com/harishsg993010/DamnVulnerableLLMProject), Large Language Model designed for hacking challenges  
- [Gandalf Lakera](https://gandalf.lakera.ai/), Prompt Injection CTF playground  
- [Vigil](https://github.com/deadbits/vigil-llm), LLM prompt injection and security scanner  
- [PALLMs](https://github.com/mik0w/pallms), Collection of payloads for attacking LLMs  
- [AI-exploits](https://github.com/protectai/ai-exploits), Exploits for MLOps systems beyond just prompt injections  
- [Offensive ML Playbook](https://wiki.offsecml.com/Welcome+to+the+Offensive+ML+Playbook), Notes on ML attacks and pentesting  
- [AnonLLM](https://github.com/fsndzomga/anonLLM), Anonymization tool for LLM APIs  
- [AI Goat](https://github.com/dhammon/ai-goat), Vulnerable LLM CTF challenges  
- [Pyrit](https://github.com/Azure/PyRIT), Python Risk Identification Tool for generative AI  
- [Raze to the Ground](https://github.com/advmlphish/raze_to_the_ground_aisec23), Adversarial HTML Attacks on ML Phishing Webpage Detectors  
- [Giskard](https://github.com/Giskard-AI/giskard), Open-source testing tool for LLM applications  
- [Safetensors](https://github.com/huggingface/safetensors), Safe serialization format for ML models  
- [Citadel Lens](https://www.citadel.co.jp/en/products/lens/), Quality testing of models according to industry standards  
- [Model-Inversion-Attack-ToolBox](https://github.com/ffhibnese/Model-Inversion-Attack-ToolBox), Framework for implementing Model Inversion attacks  
- [NeMo-Guardrails](https://github.com/NVIDIA/NeMo-Guardrails), Tool for adding guardrails between app code and LLMs  
- [AugLy](https://github.com/facebookresearch/AugLy), Tool for generating adversarial attacks  
- [Knockoffnets](https://github.com/tribhuvanesh/knockoffnets), BlackBox attack PoC for stealing model data  
- [Robust Intelligence Continuous Validation](https://www.robustintelligence.com/platform/continuous-validation), Tool for continuous model validation and compliance checking  
- [VGER](https://github.com/JosephTLucas/vger), Jupyter Attack framework  
- [AIShield Watchtower](https://github.com/bosch-aisecurity-aishield/watchtower), AI model security scanner from AIShield  
- [PS-fuzz](https://github.com/prompt-security/ps-fuzz), Tool for scanning LLM vulnerabilities  
- [Mindgard-cli](https://github.com/Mindgard/cli/), CLI-based AI security scanner  
- [PurpleLLama3](https://meta-llama.github.io/PurpleLlama/), Meta LLM Benchmark for LLM security evaluation  
- [Model Transparency](https://github.com/sigstore/model-transparency), Tool for generating model signing  
- [ARTkit](https://github.com/BCG-X-Official/artkit), Automated prompt-based testing and evaluation for Gen AI apps  
- [LangBiTe](https://github.com/SOM-Research/LangBiTe), Bias Tester framework for LLMs  
- [OpenDP](https://github.com/opendp/opendp), Core library for differential privacy algorithms  
- [TF-encrypted](https://tf-encrypted.io/), Encryption framework for TensorFlow  
- [Agentic Security](https://github.com/msoedov/agentic_security), LLM vulnerability scanner and AI red teaming kit  
- [ModelScan](https://github.com/protectai/modelscan), Protection Against ML Model Serialization Attacks
- [AI-Infra-Guard](https://github.com/Tencent/AI-Infra-Guard), AI infrastructure security assessment tool designed to discover and detect potential security risks in AI systems
