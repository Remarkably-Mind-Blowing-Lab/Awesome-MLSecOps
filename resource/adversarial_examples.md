# Adversarial Examples

- [Typographic Attacks in a Multi-Image Setting](https://arxiv.org/abs/2502.08193), explores adversarial attacks using text across multiple images.
- [Topic-FlipRAG: Topic-Orientated Adversarial Opinion Manipulation Attacks to Retrieval-Augmented Generation Models](https://arxiv.org/abs/2502.01386), investigates attacks flipping topics to alter opinions generated by RAG models.
- [Rerouting LLM Routers](https://arxiv.org/abs/2501.01818), discusses adversarial rerouting of routing mechanisms within large language models (LLMs).
- [Image-based Multimodal Models as Intruders: Transferable Multimodal Attacks on Video-based MLLMs](https://arxiv.org/abs/2501.01042), proposes transferable attacks from images to video multimodal models.
- [Robust image classification with multi-modal large language models](https://arxiv.org/abs/2412.10353), focuses on enhancing image classification robustness using multimodal LLMs.
- [Doubly-Universal Adversarial Perturbations: Deceiving Vision-Language Models Across Both Images and Text with a Single Perturbation](https://arxiv.org/abs/2412.08108), introduces perturbations universally effective on images and text simultaneously.
- [Unlocking The Potential of Adaptive Attacks on Diffusion-Based Purification](https://arxiv.org/abs/2411.16598), studies adaptive adversarial methods targeting diffusion-based purification defenses.
- [Adversarial Robustness of In-Context Learning in Transformers for Linear Regression](https://arxiv.org/abs/2411.05189), assesses transformer models' vulnerability to adversarial attacks in regression tasks.
- [Break the Visual Perception: Adversarial Attacks Targeting Encoded Visual Tokens of Large Vision-Language Models](https://arxiv.org/abs/2410.06699), analyzes adversarial attacks targeting visual encoding in vision-language models.
- [Adversarial Attacks of Vision Tasks in the Past 10 Years: A Survey](https://arxiv.org/abs/2410.23687), surveys adversarial attack methods on computer vision over the last decade.
- [Natural Language Induced Adversarial Images](https://arxiv.org/abs/2410.08620), describes adversarial image creation driven by natural language descriptions.
- [Cheating Automatic LLM Benchmarks: Null Models Achieve High Win Rates](https://arxiv.org/abs/2410.07137), examines trivial models achieving high scores on LLM benchmarks.
- [TaeBench: Improving Quality of Toxic Adversarial Examples](https://arxiv.org/abs/2410.05573), proposes methods to enhance the quality of toxic adversarial text examples.
- [Real-world Adversarial Defense against Patch Attacks based on Diffusion Model](https://arxiv.org/abs/2409.09406), introduces a diffusion-model-based defense against adversarial patch attacks.
- [Iterative Window Mean Filter: Thwarting Diffusion-based Adversarial Purification](https://arxiv.org/abs/2408.10673), proposes filtering techniques to counter diffusion-based purification.
- [Breaking Agents: Compromising Autonomous LLM Agents Through Malfunction Amplification](https://arxiv.org/abs/2407.20859), explores compromising autonomous LLM agents by amplifying minor errors.
- [Adversarial Attacks and Defenses on Text-to-Image Diffusion Models: A Survey](https://arxiv.org/abs/2407.15861), surveys adversarial techniques targeting text-to-image generative models.
- [MaPPing Your Model: Assessing the Impact of Adversarial Attacks on LLM-based Programming Assistants](https://arxiv.org/abs/2407.11072), evaluates adversarial vulnerabilities in coding-assistant LLMs.
- [Black-Box Opinion Manipulation Attacks to Retrieval-Augmented Generation of Large Language Models](https://arxiv.org/abs/2407.13757), details opinion manipulation via black-box attacks on retrieval-augmented LLMs.
- [Self-Evaluation as a Defense Against Adversarial Attacks on LLMs](https://arxiv.org/abs/2407.03234), introduces self-assessment techniques as defense against adversarial attacks.
- [SOS! Soft Prompt Attack Against Open-Source Large Language Models](https://arxiv.org/abs/2407.03160), investigates soft-prompt adversarial attacks on open-source language models.
- [Investigating and Defending Shortcut Learning in Personalized Diffusion Models](https://arxiv.org/abs/2406.18944), explores defense mechanisms against shortcut learning in diffusion models.
- [Adversarial Attacks on Multimodal Agents](https://arxiv.org/abs/2406.12814), examines adversarial vulnerabilities in multimodal AI agents.
- [Large Language Model Sentinel: Advancing Adversarial Robustness by LLM Agent](https://arxiv.org/abs/2405.20770), proposes an LLM-based sentinel agent to enhance robustness.
- [Unveiling the Achilles' Heel of NLG Evaluators: A Unified Adversarial Framework Driven by Large Language Models](https://arxiv.org/abs/2405.14646), reveals weaknesses in natural-language generation evaluators through adversarial LLM frameworks.
- [SpeechGuard: Exploring the Adversarial Robustness of Multimodal Large Language Models](https://arxiv.org/abs/2405.08317), investigates the robustness of multimodal language models incorporating speech.
- [Perturbing Attention Gives You More Bang for the Buck: Subtle Imaging Perturbations That Efficiently Fool Customized Diffusion Models](https://arxiv.org/abs/2404.15081), describes subtle image perturbations exploiting attention mechanisms.
- [Cross-Lingual Transfer Robustness to Lower-Resource Languages on Adversarial Datasets](https://arxiv.org/abs/2403.20056), evaluates robustness of cross-lingual models in low-resource language adversarial settings.
- [Improving the Robustness of Large Language Models via Consistency Alignment](https://arxiv.org/abs/2403.14221), presents methods to improve LLM robustness through consistency alignment.
- [Evaluating Robustness of Generative Search Engine on Adversarial Factual Questions](https://arxiv.org/abs/2403.12077), assesses generative search engines' robustness against adversarial questioning.
- [SSCAE -- Semantic, Syntactic, and Context-aware natural language Adversarial Examples generator](https://arxiv.org/abs/2403.11833), develops a generator for linguistically-aware adversarial text examples.
- [Transferable Multimodal Attack on Vision-Language Pre-training Models](https://www.computer.org/csdl/proceedings-article/sp/2024/313000a102/1Ub239H4xyg), proposes transferable multimodal adversarial attacks on vision-language models.
- [AVIBench: Towards Evaluating the Robustness of Large Vision-Language Model on Adversarial Visual-Instructions](https://arxiv.org/abs/2403.09346), presents a benchmark evaluating vision-language models against adversarial visual instructions.
- [The Impact of Quantization on the Robustness of Transformer-based Text Classifiers](https://arxiv.org/abs/2403.05365), studies the effect of quantization on robustness in transformer-based text classifiers.
