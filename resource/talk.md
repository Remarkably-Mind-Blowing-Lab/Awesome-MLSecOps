# Talk & Blog

- [Practical LLM Security: Takeaways From a Year in the Trenches](https://www.blackhat.com/us-24/briefings/schedule/#practical-llm-security-takeaways-from-a-year-in-the-trenches-39468), offers insights from a year of real-world LLM security challenges and solutions
- [Isolation or Hallucination? Hacking AI Infrastructure Providers for Fun and Weights](https://www.blackhat.com/us-24/briefings/schedule/#isolation-or-hallucination-hacking-ai-infrastructure-providers-for-fun-and-weights-40569), explores vulnerabilities in AI infrastructure that could lead to security breaches
- [From MLOps to MLOops - Exposing the Attack Surface of Machine Learning Platforms](https://www.blackhat.com/us-24/briefings/schedule/#from-mlops-to-mloops---exposing-the-attack-surface-of-machine-learning-platforms-39309), examines security risks across MLOps platforms and introduces new attack vectors 
- [LLM4Shell: Discovering and Exploiting RCE Vulnerabilities in Real-World LLM-Integrated Frameworks and Apps](https://www.blackhat.com/asia-24/briefings/schedule/index.html#llmshell-discovering-and-exploiting-rce-vulnerabilities-in-real-world-llm-integrated-frameworks-and-apps-37215), identifies critical RCE vulnerabilities in LLM-integrated frameworks and apps 
- [Confused Learning: Supply Chain Attacks through Machine Learning Models](https://www.blackhat.com/asia-24/briefings/schedule/index.html#confused-learning-supply-chain-attacks-through-machine-learning-models-37794), discusses the risks of supply chain attacks through machine learning model vulnerabilities
- [How to Make Hugging Face to Hug Worms: Discovering and Exploiting Unsafe Pickle.loads over Pre-Trained Large Model Hubs](https://www.blackhat.com/asia-24/briefings/schedule/index.html#how-to-make-hugging-face-to-hug-worms-discovering-and-exploiting-unsafe-pickleloads-over-pre-trained-large-model-hubs-36261), investigates the dangers of unsafe pickle.loads in pre-trained LLMs
- [Assessing the Vulnerabilities of the Open-Source Artificial Intelligence (AI) Landscape: A Large-Scale Analysis of the Hugging Face Platform](https://aivillage.org/assets/AIVDC31/DSAIL%20DEFCON%20AI%20Village.pdf), provides an in-depth analysis of vulnerabilities in Hugging Face's open-source AI landscape
- [You Sound Confused, Anyways - Thanks for The Jewels](https://aivillage.org/assets/AIVDC31/AIVDC31.pdf), explores AI vulnerabilities in real-world applications and challenges with machine learning models
- [What is MLSecOps](https://themlsecopshacker.com/p/what-is-mlsecops), an introduction to MLSecOps, covering its definition and key practices
- [Red-Teaming Large Language Models](https://huggingface.co/blog/red-teaming), explores red-teaming techniques for testing the security of LLMs
- [Google's AI red-team](https://blog.google/technology/safety-security/googles-ai-red-team-the-ethical-hackers-making-ai-safer/), discusses Google's AI red-team and their role in improving AI safety and security
- [The MLSecOps Top 10 vulnerabilities](https://ethical.institute/security.html), highlights the top 10 vulnerabilities in the MLSecOps field and their potential impacts
- [Token Smuggling Jailbreak via Adversarial Prompt](https://www.piratewires.com/p/gpt4-token-smuggling), details a jailbreak attack using adversarial prompts to smuggle tokens in LLMs
- [Just How Toxic is Data Poisoning? A Unified Benchmark for Backdoor and Data Poisoning Attacks](https://arxiv.org/pdf/2006.12557.pdf), presents a benchmark to assess the severity of data poisoning attacks in machine learning models
- [We need a new way to measure AI security](https://blog.trailofbits.com/2023/03/14/ai-security-safety-audit-assurance-heidy-khlaaf-odd/), discusses the need for a new approach to measuring AI security and safety
- [PrivacyRaven: Implementing a proof of concept for model inversion](https://blog.trailofbits.com/2021/11/09/privacyraven-implementing-a-proof-of-concept-for-model-inversion/), implements a proof of concept for model inversion attacks on machine learning models
- [Adversarial Prompts Engineering](https://github.com/dair-ai/Prompt-Engineering-Guide/blob/main/guides/prompts-adversarial.md), guide on engineering adversarial prompts for testing language model security
- [TextAttack: A Framework for Adversarial Attacks, Data Augmentation, and Adversarial Training in NLP](https://arxiv.org/abs/2005.05909), presents TextAttack, a framework for adversarial attacks and training in NLP
- [Trail Of Bits' audit of Hugging Face's safetensors library](https://github.com/trailofbits/publications/blob/master/reviews/2023-03-eleutherai-huggingface-safetensors-securityreview.pdf), provides a detailed audit of the safetensors library for security vulnerabilities
- [OWASP Top 10 for Large Language Model Applications](https://owasp.org/www-project-top-10-for-large-language-model-applications/descriptions/), an overview of the OWASP Top 10 risks for large language model applications
- [LLM Security](https://llmsecurity.net/), a dedicated site for LLM security research, covering vulnerabilities and defenses
- [Is your MLOps infrastructure leaking secrets?](https://hackstery.com/2023/10/13/no-one-is-prefect-is-your-mlops-infrastructure-leaking-secrets/), explores how MLOps infrastructure can leak secrets and how to prevent it
- [Embrace The Red, blog where show how u can hack LLM's](https://embracethered.com/), a blog focused on teaching how to hack LLMs using red-team tactics
- [Audio-jacking: Using generative AI to distort live audio transactions](https://securityintelligence.com/posts/using-generative-ai-distort-live-audio-transactions/), explores the potential for AI to distort live audio communications and its implications
- [HADESS - Web LLM Attacks](https://hadess.io/web-llm-attacks/), discusses web-based attacks targeting large language models
- [WTF-blog - MLSecOps frameworks ... Which ones are available and what is the difference?](https://blog.wearetyomsmnv.wtf/articles/mlsecops-frameworks-...-which-ones-are-available-and-what-is-the-difference), compares available MLSecOps frameworks and explains their differences
- [DreadNode Paper Stack](https://dreadnode.notion.site/2582fe5306274c60b85a5e37cf99da7e?v=74ab79ed1452441dab8a1fa02099fed), a curated list of papers on AI and security by DreadNode
