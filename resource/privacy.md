# Privacy

- [The Canary's Echo: Auditing Privacy Risks of LLM-Generated Synthetic Text](https://arxiv.org/abs/2502.14921), Investigates how synthetic text generated by LLMs may expose privacy risks.  
- [A General Pseudonymization Framework for Cloud-Based LLMs: Replacing Privacy Information in Controlled Text Generation](https://arxiv.org/abs/2502.15233), Proposes a pseudonymization framework for protecting privacy in cloud-based LLMs.  
- [Unveiling Privacy Risks in LLM Agent Memory](https://arxiv.org/abs/2502.13172), Explores privacy vulnerabilities in LLM-based agent memory.  
- [Mimicking the Familiar: Dynamic Command Generation for Information Theft Attacks in LLM Tool-Learning System](https://arxiv.org/abs/2502.11358), Analyzes how adversaries can use dynamic command generation to steal information.
- [PrivAgent: Agentic-based Red-teaming for LLM Privacy Leakage](https://arxiv.org/abs/2412.05734), Uses red-teaming approaches to evaluate LLM privacy leakage risks.  
- [VLSBench: Unveiling Visual Leakage in Multimodal Safety](https://arxiv.org/abs/2411.19939), Establishes a benchmark for detecting visual privacy leakage in multimodal AI models.  
- [Can Humans Oversee Agents to Prevent Privacy Leakage?](https://arxiv.org/abs/2411.01344), Investigates human oversight in preventing LLM agents from leaking private data.  
- [Empowering Users in Digital Privacy Management through Interactive LLM-Based Agents](https://arxiv.org/abs/2410.11906), Proposes interactive agents to improve user control over digital privacy.  
- [LLM-PBE: Assessing Data Privacy in Large Language Models](https://arxiv.org/abs/2408.12787), A framework for evaluating how LLMs handle private data.  
- [Mitigating Privacy Seesaw in Large Language Models](https://aclanthology.org/2024.findings-acl.315/), Introduces privacy neuron editing techniques to balance privacy preservation.  
- [Reducing Privacy Risks in Online Self-Disclosures with Language Models](https://aclanthology.org/2024.acl-long.741/), Studies how LLMs influence privacy risks in self-disclosure scenarios.  
- [Preserving Privacy in Large Language Models: A Survey on Current Threats and Solutions](https://arxiv.org/abs/2408.05212), A comprehensive survey of privacy threats and mitigation strategies in LLMs.  
- [Mitigating the Privacy Issues in Retrieval-Augmented Generation (RAG) via Pure Synthetic Data](https://arxiv.org/abs/2406.14773), Proposes using synthetic data to reduce privacy risks in RAG models.  
- [Information Leakage from Embedding in Large Language Models](https://arxiv.org/abs/2405.11916), Studies privacy risks arising from embedding models.  
- [Excuse me, sir? Your language model is leaking (information)](https://arxiv.org/abs/2401.10360), Examines information leakage issues in LLMs.  
