# Privacy

- [The Canary's Echo: Auditing Privacy Risks of LLM-Generated Synthetic Text](https://arxiv.org/abs/2502.14921), Investigates how synthetic text generated by LLMs may expose privacy risks.  
- [A General Pseudonymization Framework for Cloud-Based LLMs: Replacing Privacy Information in Controlled Text Generation](https://arxiv.org/abs/2502.15233), Proposes a pseudonymization framework for protecting privacy in cloud-based LLMs.  
- [Unveiling Privacy Risks in LLM Agent Memory](https://arxiv.org/abs/2502.13172), Explores privacy vulnerabilities in LLM-based agent memory.  
- [Mimicking the Familiar: Dynamic Command Generation for Information Theft Attacks in LLM Tool-Learning System](https://arxiv.org/abs/2502.11358), Analyzes how adversaries can use dynamic command generation to steal information.
- [PrivAgent: Agentic-based Red-teaming for LLM Privacy Leakage](https://arxiv.org/abs/2412.05734), Uses red-teaming approaches to evaluate LLM privacy leakage risks.  
- [VLSBench: Unveiling Visual Leakage in Multimodal Safety](https://arxiv.org/abs/2411.19939), Establishes a benchmark for detecting visual privacy leakage in multimodal AI models.  
- [Can Humans Oversee Agents to Prevent Privacy Leakage?](https://arxiv.org/abs/2411.01344), Investigates human oversight in preventing LLM agents from leaking private data.  
- [Empowering Users in Digital Privacy Management through Interactive LLM-Based Agents](https://arxiv.org/abs/2410.11906), Proposes interactive agents to improve user control over digital privacy.  
- [LLM-PBE: Assessing Data Privacy in Large Language Models](https://arxiv.org/abs/2408.12787), A framework for evaluating how LLMs handle private data.  
- [Mitigating Privacy Seesaw in Large Language Models](https://aclanthology.org/2024.findings-acl.315/), Introduces privacy neuron editing techniques to balance privacy preservation.  
- [Reducing Privacy Risks in Online Self-Disclosures with Language Models](https://aclanthology.org/2024.acl-long.741/), Studies how LLMs influence privacy risks in self-disclosure scenarios.  
- [Preserving Privacy in Large Language Models: A Survey on Current Threats and Solutions](https://arxiv.org/abs/2408.05212), A comprehensive survey of privacy threats and mitigation strategies in LLMs. 
- [Mitigating the Privacy Issues in Retrieval-Augmented Generation (RAG) via Pure Synthetic Data](https://arxiv.org/abs/2406.14773), Proposes using synthetic data to reduce privacy risks in RAG models.  
- [Information Leakage from Embedding in Large Language Models](https://arxiv.org/abs/2405.11916), Studies privacy risks arising from embedding models.  
- [Excuse me, sir? Your language model is leaking (information)](https://arxiv.org/abs/2401.10360), Examines information leakage issues in LLMs.  
- [A Survey of Privacy Attacks in Machine Learning](https://dl.acm.org/doi/10.1145/3624010), Comprehensive survey on different privacy attack methods in ML systems.  
- [SoK: Model Inversion Attack Landscape](https://ieeexplore.ieee.org/abstract/document/10221914), A systematic classification of model inversion attacks with challenges and future directions.  
- [On the Inadequacy of Similarity-based Privacy Metrics](https://arxiv.org/abs/2312.05114), Analyzes the weaknesses of existing privacy metrics in synthetic data.  
- [PrivacyRaven](https://github.com/trailofbits/PrivacyRaven), A library for auditing ML models against privacy attacks.  
- [TensorFlow Privacy](https://github.com/tensorflow/privacy/tree/master/tensorflow_privacy/privacy/membership_inference_attack), TensorFlow's framework for membership inference testing.  
- [Machine Learning Privacy Meter](https://github.com/privacytrustlab/ml_privacy_meter), A privacy risk assessment tool.  
- [Adversarial Robustness Toolbox (ART)](https://github.com/Trusted-AI/adversarial-robustness-toolbox), Privacy and adversarial attack evaluation for ML.  
- [Membership Inference via Backdooring](https://arxiv.org/abs/2206.04823), Injecting backdoors to exploit membership inference vulnerabilities.  
- [Enhanced Membership Inference Attacks](https://arxiv.org/abs/2111.09679), New attack strategies to infer training membership more effectively.  
- [Membership Inference on Word Embeddings](https://arxiv.org/abs/2106.11384), Examines privacy risks in word embeddings and NLP models.  
- [Source Inference Attacks in Federated Learning](https://arxiv.org/abs/2109.05659), Source-tracing attacks in FL systems. 
- [Analysis of Hidden Information in Model Inversion](https://ieeexplore.ieee.org/document/10184490), Exploiting hidden information in neural networks for data reconstruction. 
- [Model Inversion Attack with Least Information](https://ieeexplore.ieee.org/abstract/document/10136179), A lightweight yet effective inversion attack technique.  
- [Text Embeddings Reveal (Almost) As Much As Text](https://arxiv.org/abs/2310.06816?ref=upstract.com), Investigates privacy leakage from text embeddings.  
- [Attribute Inference on Synthetic Data](https://arxiv.org/abs/2301.10053), Linear reconstruction approaches for inferring private attributes.  
